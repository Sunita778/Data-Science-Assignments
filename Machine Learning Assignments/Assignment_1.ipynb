{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_1\n",
    "Submitted by - Sunita Pradhan\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What does one mean by the term \"machine learning\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Machine learning is a field of computer science and artificial intelligence that focuses on designing algorithms and models that allow machines to learn and make predictions or decisions based on data, without being explicitly programmed to do so. In other words, it is a technique of teaching computers to learn from data, without being explicitly programmed to do so.\n",
    "\n",
    "Machine learning involves training models on large datasets, allowing them to identify patterns and relationships in the data, and then using those patterns to make predictions or decisions on new data. Machine learning models can be used in a variety of applications, such as image recognition, speech recognition, natural language processing, recommender systems, and many others.\n",
    "\n",
    "There are several different types of machine learning, including supervised learning, unsupervised learning, and reinforcement learning, each with its own set of techniques and algorithms. Supervised learning involves training models on labeled data, while unsupervised learning involves training models on unlabeled data. Reinforcement learning involves training models through a system of rewards and punishments based on their actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Can you think of 4 distinct types of issues where it shines?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "- `Image recognition`: Machine learning is very effective in recognizing and identifying objects and patterns in images. This has many applications, such as facial recognition, medical imaging, and autonomous vehicles.\n",
    "- `Natural language processing`: Machine learning is also very effective in processing natural language data, such as speech and text. This has many applications, such as virtual assistants, chatbots, and sentiment analysis.\n",
    "- `Fraud detection`: Machine learning is very effective in detecting fraudulent activities, such as credit card fraud and identity theft. This is because machine learning algorithms can quickly identify patterns and anomalies in large datasets that would be difficult for humans to spot.\n",
    "- `Personalization`: Machine learning is very effective in creating personalized experiences for users, such as personalized recommendations on e-commerce websites, personalized news feeds, and personalized marketing campaigns. This is because machine learning algorithms can quickly analyze user data and make predictions about what products or services a user is likely to be interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.What is a labeled training set, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "A labeled training set is a dataset used in supervised machine learning, where the data is already labeled with the correct output or target variable. The labeled training set is used to train a machine learning model to make accurate predictions or decisions on new, unseen data by adjusting its internal parameters to minimize the difference between its predicted outputs and the true outputs in the labeled training set. Once the training is complete, the model is evaluated on a separate dataset called the test set to measure its ability to generalize to new, unseen data.\n",
    "\n",
    "OR\n",
    "\n",
    "We can say, a labeled training set is a dataset with known outputs used to train a machine learning model, and the model learns to make accurate predictions on new data by adjusting its internal parameters to minimize the difference between its predicted outputs and the true outputs in the labeled training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.What are the two most important tasks that are supervised?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "In supervised machine learning, there are two primary tasks: classification and regression.\n",
    "\n",
    "- `Classification`: In classification, the goal is to predict the class or category of a given input. For example, given an image of a fruit, the goal might be to classify it as an apple, banana, or orange. Classification models are trained on labeled data where the input data is associated with a specific class or category.\n",
    "\n",
    "- `Regression`: In regression, the goal is to predict a continuous numerical output value. For example, given data on a person's age, height, and weight, the goal might be to predict their blood pressure. Regression models are trained on labeled data where the input data is associated with a specific numerical output value.\n",
    "\n",
    "Both classification and regression are important tasks in supervised machine learning, and they can be used in a variety of applications such as image recognition, speech recognition, medical diagnosis, and financial forecasting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Can you think of four examples of unsupervised tasks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Unsupervised machine learning tasks are used to find patterns and structure in data without predefined labels or categories.\n",
    "- `Clustering`: Clustering is a task in which the goal is to group similar data points together into clusters based on their features. Clustering is an unsupervised task because there are no predefined labels or categories for the data points. Clustering can be used for applications such as customer segmentation, image segmentation, and anomaly detection.\n",
    "\n",
    "- `Dimensionality Reduction`: Dimensionality reduction is a task in which the goal is to reduce the number of features in a dataset while preserving its underlying structure. Dimensionality reduction can be used to visualize high-dimensional data, speed up training times for machine learning models, and improve model performance.\n",
    "\n",
    "- `Association Rule Mining`: Association rule mining is a task in which the goal is to find patterns or relationships between items in a dataset. This is often used in market basket analysis, where the goal is to identify which products are frequently bought together.\n",
    "\n",
    "- `Anomaly detection`: Anomaly detection involves identifying data points that are significantly different from the majority of the data. Anomaly detection can be used in fraud detection, intrusion detection, and quality control in manufacturing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.State the machine learning model that would be best to make a robot walk through various unfamiliar terrains?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The machine learning model that would be best to make a robot walk through various unfamiliar terrains is a type of reinforcement learning called policy learning.\n",
    "\n",
    "In policy learning, the robot learns a policy, which is a mapping from states to actions, that maximizes some reward signal. The robot explores its environment by taking actions and receiving feedback in the form of a reward signal. The reward signal provides feedback to the robot about whether its actions are good or bad, and the robot adjusts its policy to maximize the reward signal.\n",
    "\n",
    "To train the policy, the robot can use a simulation environment that models various terrains. The robot can then explore and learn from the simulation, before being deployed in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Which algorithm will you use to divide your customers into different groups?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "To divide customers into different groups, a clustering algorithm would be appropriate. Clustering is an unsupervised learning technique that involves grouping together similar data points based on their similarity in some feature space.\n",
    "\n",
    "One commonly used clustering algorithm is k-means clustering. K-means clustering involves partitioning the data into k clusters, where k is a user-defined parameter. The algorithm works by randomly selecting k initial centroids, assigning each data point to the nearest centroid, and then recalculating the centroids based on the new cluster assignments. The process is repeated until the centroids converge to a stable configuration.\n",
    "\n",
    "Another popular clustering algorithm is hierarchical clustering, which involves creating a hierarchy of nested clusters. Hierarchical clustering can be agglomerative, where each data point is initially treated as a separate cluster and then merged together based on some similarity metric, or divisive, where all data points are initially treated as one cluster and then recursively divided into smaller clusters.\n",
    "\n",
    "The choice of algorithm depends on the specific characteristics of the data and the clustering problem at hand. Other factors to consider include the number of clusters desired, the size and dimensionality of the dataset, and the interpretability of the resulting clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Will you consider the problem of spam detection to be a supervised or unsupervised learning problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The problem of spam detection is typically considered to be a supervised learning problem.\n",
    "\n",
    "In supervised learning, the goal is to train a machine learning model to make accurate predictions based on labeled data. In the case of spam detection, the labeled data would consist of examples of both spam and non-spam (ham) emails, and the machine learning model would be trained to classify new, unseen emails as either spam or ham.\n",
    "\n",
    "Supervised learning algorithms commonly used for spam detection include logistic regression, decision trees, random forests, support vector machines (SVMs), and neural networks. These algorithms learn from labeled data by adjusting their internal parameters to minimize the difference between their predicted outputs and the true outputs in the labeled training set.\n",
    "\n",
    "Unsupervised learning algorithms such as clustering could potentially be used for spam detection, but they would likely be used as a pre-processing step to identify patterns or clusters in the data that could then be used to train a supervised learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.What is the concept of an online learning system?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "An online learning system is a type of machine learning system that can learn and adapt to new data in real-time, as the data arrives. In online learning, the model is updated continuously as new data becomes available, allowing the model to adapt to changes in the underlying data distribution over time.\n",
    "\n",
    "Online learning is useful in applications where the data is continuously streaming or arriving at a high rate, and where the underlying data distribution may be changing over time. Some examples of online learning applications include fraud detection, recommendation systems, and online advertising.\n",
    "\n",
    "In an online learning system, the model typically starts with some initial parameters or weights and is updated incrementally as new data arrives. The update process is usually done using stochastic gradient descent or a similar optimization algorithm, which updates the model parameters in small batches based on the most recent data.\n",
    "\n",
    "One key advantage of online learning is that it can be more efficient than batch learning, as it avoids the need to store and process large amounts of historical data. Online learning can also be more flexible, as it can adapt to changing data distributions and can incorporate new data quickly.\n",
    "\n",
    "However, online learning also has some challenges, such as the need to carefully balance model stability and adaptability, and the potential for the model to be affected by data drift or concept drift over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.What is out-of-core learning, and how does it differ from core learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Out-of-core learning, also known as \"out-of-memory\" learning, is a technique used in machine learning to train models on datasets that are too large to fit into the memory of a single computer.\n",
    "\n",
    "In out-of-core learning, the data is read in small batches from disk or a distributed file system, and the model is updated incrementally on each batch. This allows the model to learn from the entire dataset, without having to load it all into memory at once.\n",
    "\n",
    "Out-of-core learning differs from core learning, also known as in-memory learning, in that the latter assumes that the entire dataset can fit into the memory of a single machine. Core learning algorithms can access the entire dataset in a single pass, which can be more efficient than out-of-core learning if the dataset is small enough to fit into memory.\n",
    "\n",
    "However, as datasets grow in size, it becomes increasingly difficult or impossible to fit them into memory using core learning algorithms. This is where out-of-core learning becomes important, as it allows models to be trained on datasets that are much larger than what can fit into memory.\n",
    "\n",
    "Out-of-core learning techniques are commonly used in applications such as natural language processing, image recognition, and recommendation systems, where the datasets can be extremely large and are constantly growing. Examples of out-of-core learning algorithms include stochastic gradient descent and mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.What kind of learning algorithm makes predictions using a similarity measure?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "A learning algorithm that makes predictions using a similarity measure is known as a nearest neighbor algorithm.\n",
    "\n",
    "In nearest neighbor algorithms, the prediction for a new example is based on the similarity between the new example and the examples in the training dataset. The most common similarity measure used is the Euclidean distance, which calculates the distance between two points in a multi-dimensional space.\n",
    "\n",
    "The nearest neighbor algorithm works by first selecting a number (k) of examples from the training dataset that are closest to the new example in terms of the similarity measure. The algorithm then makes a prediction based on the labels of the k-nearest neighbors. For example, in a classification problem, the prediction might be the majority class among the k-nearest neighbors.\n",
    "\n",
    "Nearest neighbor algorithms are often used in cases where the data is high-dimensional or where the underlying relationship between the features and labels is complex and difficult to model using traditional statistical or machine learning methods. However, nearest neighbor algorithms can be computationally expensive and may suffer from overfitting if the dataset is too small or too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.What's the difference between a model parameter and a hyperparameter in a learning algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "In a machine learning algorithm, a model parameter is a parameter that is learned by the algorithm during the training process, whereas a hyperparameter is a parameter that is set before the training process begins.\n",
    "\n",
    "Model parameters are the internal parameters of the model that define how it maps inputs to outputs. For example, in a linear regression model, the model parameters are the coefficients that define the line or plane that best fits the data. In a neural network model, the model parameters are the weights and biases that define the connections between the neurons.\n",
    "\n",
    "Hyperparameters, on the other hand, are parameters that are set externally to the model and control aspects of the learning process. Examples of hyperparameters include the learning rate, which controls the step size used in the gradient descent optimization algorithm, or the number of hidden layers in a neural network.\n",
    "\n",
    "Unlike model parameters, hyperparameters are not learned during the training process, but are instead set based on prior knowledge or through trial and error. The choice of hyperparameters can have a significant impact on the performance of the model, and finding the optimal set of hyperparameters is often a challenging problem.\n",
    "\n",
    "In summary, *model parameters are learned by the algorithm during training, while hyperparameters are set externally and control aspects of the learning process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13.What are the criteria that model-based learning algorithms look for? What is the most popular method they use to achieve success? What method do they use to make predictions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Model-based learning algorithms look for a model that can generalize well to new, unseen data. To achieve this goal, they typically seek to minimize a chosen objective function or error metric on the training data, subject to constraints such as model complexity or sparsity.\n",
    "\n",
    "The most popular method that model-based learning algorithms use to achieve success is to fit a flexible and expressive model to the training data, such as a neural network or a decision tree. They then use various regularization techniques, such as L1 or L2 regularization, to prevent overfitting and improve generalization performance.\n",
    "\n",
    "Once the model is trained, model-based learning algorithms use it to make predictions on new, unseen data by applying the learned function to the input features. The specific method they use to make predictions depends on the type of model and the task at hand. For example, linear models use a weighted sum of the input features, while neural networks use a series of nonlinear transformations to map the input to the output.\n",
    "\n",
    "Overall, the criteria that model-based learning algorithms look for is a model that can generalize well to new, unseen data, and they achieve success by fitting a flexible and expressive model to the training data while using regularization techniques to prevent overfitting. They make predictions on new data by applying the learned function to the input features, using different methods depending on the type of model and the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.Can you name four of the most important Machine Learning challenges?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "- `Data quality and quantity`: Machine learning algorithms require a large amount of high-quality data to train and generalize well. However, collecting and cleaning data can be a challenging and time-consuming task.\n",
    "\n",
    "- `Model selection and hyperparameter tuning`: Choosing the right model and hyperparameters for a given task can greatly affect the performance of a machine learning algorithm. However, with so many models and hyperparameters to choose from, it can be difficult to find the best combination.\n",
    "\n",
    "- `Overfitting and underfitting`: Overfitting occurs when a model is too complex and learns noise in the training data, leading to poor generalization performance on new data. Underfitting occurs when a model is too simple and fails to capture the underlying patterns in the data, leading to poor performance on both the training and test data.\n",
    "\n",
    "- `Interpretability and explainability`: As machine learning is increasingly used in real-world applications, there is a growing need to understand how models make predictions and to ensure that they are fair and unbiased. However, many machine learning models are complex and opaque, making it difficult to interpret their decisions and identify potential biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15.What happens if the model performs well on the training data but fails to generalize the results to new situations? Can you think of three different options?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "If the model performs well on the training data but fails to generalize to new situations, it is likely overfitting. Overfitting occurs when the model becomes too complex and starts to memorize noise in the training data rather than capturing the underlying patterns that would enable it to generalize to new situations.\n",
    "\n",
    "Here are three different options to address this problem:\n",
    "\n",
    "- `Use regularization techniques`: Regularization is a method to prevent overfitting by adding a penalty term to the objective function that discourages the model from becoming too complex. Regularization can take different forms, such as L1 or L2 regularization, dropout, or early stopping.\n",
    "\n",
    "- `Increase the amount of data`: Overfitting can occur when there is not enough data to properly train the model. By increasing the amount of data, the model will have more examples to learn from and will be less likely to memorize noise.\n",
    "\n",
    "- `Use a simpler model`: If the model is too complex for the available data, it may be necessary to use a simpler model that is less prone to overfitting. For example, a linear regression model may be more appropriate than a neural network if there are only a few input features and a limited amount of data available.\n",
    "\n",
    "By using one or more of these options, it may be possible to reduce overfitting and improve the model's ability to generalize to new situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16.What exactly is a test set, and why would you need one?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "In machine learning, a test set is a dataset that is used to evaluate the performance of a trained model. The test set is separate from the training set, which is used to train the model, and the validation set, which is used to tune the hyperparameters of the model.\n",
    "\n",
    "The test set is used to provide an estimate of how well the model will generalize to new, unseen data. This is important because the ultimate goal of a machine learning model is to make accurate predictions on new data that it has not seen before.\n",
    "\n",
    "To use the test set, the trained model is applied to the examples in the test set, and the predictions are compared to the true labels. The performance of the model is then evaluated using various metrics, such as accuracy, precision, recall, or F1 score.\n",
    "\n",
    "Having a separate test set is important because it provides an unbiased estimate of the performance of the model on new data. If the model is evaluated on the same data that was used to train it, it may overfit to the training data and not generalize well to new data.\n",
    "\n",
    "*A test set is a dataset that is used to evaluate the performance of a trained machine learning model on new, unseen data. It is necessary to have a separate test set to avoid overfitting and obtain an unbiased estimate of the model's performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17.What is a validation set's purpose?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The purpose of a validation set in machine learning is to provide a way to tune the hyperparameters of a model and prevent overfitting to the training set.\n",
    "\n",
    "During the training process, the model is fit to the training data by adjusting its internal parameters (e.g., weights in a neural network) based on the loss or error between the predicted outputs and the true outputs. However, the hyperparameters of the model, such as the learning rate or regularization strength, cannot be learned directly from the training data and must be set by the user.\n",
    "\n",
    "The validation set is used to evaluate the performance of the model on data that is not used for training, and to select the best set of hyperparameters that minimize the error on the validation set. By tuning the hyperparameters using the validation set, the model can generalize better to new, unseen data.\n",
    "\n",
    "The validation set is separate from the test set, which is used to evaluate the final performance of the model after the hyperparameters have been selected. It is important to keep the validation set separate from the test set to avoid overfitting to the test set, which would make the performance estimate overly optimistic.\n",
    "\n",
    "*The purpose of a validation set is to provide a way to tune the hyperparameters of a machine learning model and prevent overfitting to the training set. The validation set is used to select the best set of hyperparameters that minimize the error on the validation set, while the test set is used to evaluate the final performance of the model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18.What precisely is the train-dev kit, when will you need it, how do you put it to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans: *\n",
    "\n",
    "The train-dev set, also known as development set or holdout set, is a subset of the training set that is used for model selection and tuning during the development of a machine learning model.\n",
    "\n",
    "The purpose of the train-dev set is to provide a way to evaluate the performance of the model during development, without overfitting to the test set or the validation set. This is especially useful when working with small datasets, where the size of the test set or validation set can impact the performance estimate.\n",
    "\n",
    "To use the train-dev set, the original training set is split into two parts: the new training set and the train-dev set. The new training set is used to train the model, while the train-dev set is used for model selection and tuning.\n",
    "\n",
    "During the development process, different models and hyperparameters are trained on the new training set, and their performance is evaluated on the train-dev set. The model or set of hyperparameters that performs best on the train-dev set is selected as the final model, and the performance is then evaluated on the test set.\n",
    "\n",
    "It is important to keep the train-dev set separate from the validation set and the test set, as the model selection process should not be influenced by the test set. The size of the train-dev set should be large enough to provide a reliable estimate of the model performance, but not too large that it significantly reduces the size of the new training set.\n",
    "\n",
    "*The train-dev set is a subset of the training set that is used for model selection and tuning during the development of a machine learning model. It is used to evaluate the performance of different models and hyperparameters, without overfitting to the test set or the validation set. The train-dev set should be kept separate from the validation set and the test set, and its size should be chosen carefully to provide a reliable estimate of the model performance.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.What could go wrong if you use the test set to tune hyperparameters?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "If you use the test set to tune hyperparameters, you run the risk of overfitting to the test set, which can result in poor generalization performance on new, unseen data. Specifically, the problem is that you may end up selecting hyperparameters that perform well on the test set, but not necessarily on new data.\n",
    "\n",
    "The test set is typically used to evaluate the final performance of a machine learning model, after all the model development and hyperparameter tuning has been completed. By using the test set for hyperparameter tuning, you are effectively incorporating information from the test set into the model, which can lead to overly optimistic performance estimates.\n",
    "\n",
    "For example, suppose you have a dataset with 1,000 samples and you split it into a 70-15-15 train-validation-test split. If you use the test set for hyperparameter tuning, you are effectively reducing the size of the test set to just 15 samples. This can result in a noisy and unreliable estimate of the model's performance on new data.\n",
    "\n",
    "To avoid this problem, it is important to keep the test set separate from the development process, including hyperparameter tuning. Instead, you should use a separate validation set or train-dev set for model selection and hyperparameter tuning, and only evaluate the final performance of the model on the test set. This ensures that the model is not overfitting to the test set and can generalize well to new, unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
