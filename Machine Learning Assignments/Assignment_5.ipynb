{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_5\n",
    "Submitted by - Sunita Pradhan\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What are the key tasks that machine learning entails? What does data pre-processing imply?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Machine learning involves a variety of tasks that need to be carried out in order to develop accurate models that can make accurate predictions. These tasks include:\n",
    "\n",
    "`Data collection`: Gathering data from various sources such as databases, APIs, and web scraping.\n",
    "\n",
    "`Data preprocessing`: This involves cleaning, transforming, and preparing the data for analysis. It includes tasks such as removing missing values, handling outliers, scaling data, and encoding categorical variables.\n",
    "\n",
    "`Feature selection`: Selecting the most relevant features from the data to use in the model. This is important to avoid overfitting and improve model accuracy.\n",
    "\n",
    "`Model selection`: Choosing the appropriate machine learning algorithm that best suits the problem at hand.\n",
    "\n",
    "`Model training`: Using the selected algorithm to train the model on the preprocessed data.\n",
    "\n",
    "`Model evaluation`: Assessing the performance of the model by measuring its accuracy, precision, recall, and other evaluation metrics.\n",
    "\n",
    "`Model deployment`: Deploying the model in a production environment for real-world use.\n",
    "\n",
    "Data preprocessing is a crucial step in machine learning that involves cleaning and transforming the raw data into a format that can be easily analyzed by machine learning algorithms. It is necessary to ensure that the data is accurate, complete, and relevant to the problem at hand. Data preprocessing involves tasks such as data cleaning, data integration, data reduction, and data transformation. Data cleaning involves removing missing values, handling outliers, and correcting inconsistent data. Data integration involves merging data from different sources into a single dataset. Data reduction involves selecting a subset of the most relevant features to reduce the dimensionality of the dataset. Data transformation involves scaling the data to a common range or transforming it into a different representation. The goal of data preprocessing is to improve the accuracy and efficiency of the machine learning model by providing high-quality data that is free from errors and inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "`Quantitative` and `qualitative` data are two types of data used in statistics and research. Quantitative data are numerical measurements or counts, while qualitative data are non-numerical and usually categorical or descriptive. Here are some key differences between the two:\n",
    "\n",
    "`Nature`: Quantitative data are objective and based on measurable observations, while qualitative data are subjective and based on personal opinions, feelings, and perceptions.\n",
    "\n",
    "`Measurement`: Quantitative data are typically measured using instruments or tools, such as scales or meters, while qualitative data are measured through interviews, surveys, or observations.\n",
    "\n",
    "`Analysis`: Quantitative data are analyzed using statistical techniques, such as regression or correlation analysis, while qualitative data are analyzed using content analysis or grounded theory.\n",
    "\n",
    "Examples: Examples of quantitative data include age, height, weight, income, and test scores, while examples of qualitative data include gender, ethnicity, marital status, occupation, and opinions.\n",
    "\n",
    "*Quantitative data are numerical and objective, while qualitative data are categorical and subjective. Both types of data are important in research and can be used to answer different research questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The following is a basic data collection that includes some sample records.\n",
    "\n",
    "- `Determine What Information You Want to Collect`: The first thing you need to do is choose what details you want to collect. You’ll need to decide what topics the information will cover, who you want to collect it from and how much data you need. Your goals — what you hope to accomplish using your data — will determine your answers to these questions. As an example, you may decide to collect data about which type of articles are most popular on your website among visitors who are between the ages of 18 and 34. You might also choose to gather information about the average age of all of the customers who bought a product from your company within the last month.\n",
    "\n",
    "- `Set a Timeframe for Data Collection`: Next, you can start formulating your plan for how you’ll collect your data. In the early stages of your planning process, you should establish a timeframe for your data collection. You may want to gather some types of data continuously. When it comes to transactional data and website visitor data, for example, you may want to set up a method for tracking that data over the long term. If you’re tracking data for a specific campaign, however, you’ll track it over a defined period. In these instances, you’ll have a schedule for when you’ll start and end your data collection.\n",
    "\n",
    "- `Determine Your Data Collection Method`: At this step, you will choose the data collection method that will make up the core of your data-gathering strategy. To select the right collection method, you’ll need to consider the type of information you want to collect, the timeframe over which you’ll obtain it and the other aspects you determined.\n",
    "\n",
    "- `Collect the Data`: Once you have finalized your plan, you can implement your data collection strategy and start collecting data. You can store and organize your data in your DMP. Be sure to stick to your plan and check on its progress regularly. It may be useful to create a schedule for when you will check in with how your data collection is proceeding, especially if you are collecting data continuously. You may want to make updates to your plan as conditions change and you get new information.\n",
    "\n",
    "- `Analyze the Data and Implement Your Findings`: Once you’ve collected all of your data, it’s time to analyze it and organize your findings. The analysis phase is crucial because it turns raw data into valuable insights that you can use to enhance your marketing strategies, products and business decisions. You can use the analytics tools built into our DMP to help with this step. Once you’ve uncovered the patterns and insights in your data, you can implement the findings to improve your business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What are the various causes of machine learning data issues? What are the ramifications?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "There are several causes of machine learning data issues, which can have serious ramifications for the accuracy and reliability of machine learning models. Some of these causes include:\n",
    "\n",
    "`Incomplete data`: If data is missing or incomplete, it can lead to biased or inaccurate models.\n",
    "\n",
    "`Noisy data`: Data that is contaminated by errors, outliers, or anomalies can produce inaccurate or biased results.\n",
    "\n",
    "`Biased data`: If the training data is not representative of the population, it can result in biased models that do not generalize well.\n",
    "\n",
    "`Inconsistent data`: Inconsistent data, such as conflicting values or measurements, can result in inaccurate models.\n",
    "\n",
    "`Irrelevant data`: Data that is irrelevant to the problem being solved can lead to overfitting or underfitting.\n",
    "\n",
    "The ramifications of these issues can be significant. Models trained on incomplete, noisy, or biased data can produce inaccurate or biased predictions, which can have negative consequences in real-world applications. Inaccurate models can also lead to wasted resources and lost opportunities for organizations that rely on machine learning. Therefore, it is critical to address these data issues before training machine learning models to ensure the accuracy and reliability of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Demonstrate various approaches to categorical data exploration with appropriate examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Various approaches to categorical data exploration are:\n",
    "\n",
    "1. `Unique value count`: One of the first things which can be useful during data exploration is to see how many unique values are there in categorical columns. \n",
    "\n",
    "2. `Frequency Count`: Frequency count is finding how frequent individual values occur in column. \n",
    "\n",
    "3. `Variance`: Variance gives a good indication how the values are spread. \n",
    "\n",
    "4. `Pareto Analysis`: Pareto analysis is a creative way of focusing on what is important. Pareto 80–20 rule can be effectively used in data exploration. \n",
    "\n",
    "5. `Histogram`: Histogram are one of the data scientists favourite data exploration techniques. It gives information on the range of values in which most of the values fall. It also gives information on whether there is any skew in data. \n",
    "\n",
    "6. `Correlation Heat-map between all numeric columns`: The term correlation refers to a mutual relationship or association between two things. \n",
    "\n",
    "7. `Pearson Correlation and Trend between two numeric columns`: Once you have visualised correlation heat-map , the next step is to see the correlation trend between two specific numeric columns. \n",
    "\n",
    "8. `Outlier overview`: Finding something unusual in data is called Outlier detection (also known as anomaly detection). These outliers represent something unusual, rare , anomaly or something exceptional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Missing values in variables can affect the learning process of a machine learning model as it can lead to biased or inaccurate results. Some of the ways missing values can impact learning are:\n",
    "\n",
    "`Loss of data`: Missing values reduce the size of the dataset, which can result in the loss of valuable information.\n",
    "\n",
    "`Biased results`: The missing values may be associated with specific categories or groups, resulting in biased results.\n",
    "\n",
    "`Inaccurate imputations`: If the missing values are imputed with incorrect values, it can affect the accuracy of the model.\n",
    "\n",
    "--- To handle missing values, we can use various techniques such as:\n",
    "\n",
    "`Deleting observations`: If the number of missing values is small, we can remove the observations with missing values. However, if the number of missing values is large, removing observations can lead to a significant loss of information.\n",
    "\n",
    "`Imputing missing values`: We can estimate missing values using various imputation methods such as mean imputation, median imputation, mode imputation, etc. Imputation methods can help fill in the missing values, but they may also introduce noise into the data.\n",
    "\n",
    "`Treating missing values as a separate category`: We can treat the missing values as a separate category and assign a value such as \"unknown\" or \"not available\" to them.\n",
    "\n",
    "To demonstrate the exploration of categorical data with missing values, let's consider an example of a dataset containing information about customer purchases. The dataset includes variables such as customer ID, product category, and purchase amount.\n",
    "\n",
    "Suppose some of the purchase amounts are missing. To explore the categorical data, we can:\n",
    "\n",
    "- Check the frequency of missing values in the purchase amount variable.\n",
    "\n",
    "- Determine if the missing values are associated with any particular product category or customer ID.\n",
    "\n",
    "- Impute the missing values with the mean or median purchase amount for that particular product category or customer ID.\n",
    "\n",
    "- Treat the missing values as a separate category and assign a value such as \"unknown\" to them.\n",
    "\n",
    "By exploring the categorical data in this way, we can handle the missing values appropriately and improve the accuracy of the machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Describe the various methods for dealing with missing data values in depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Missing data is a common problem in machine learning, and there are various methods to handle missing values. Here are some of the most common methods:\n",
    "\n",
    "`Deletion`: Deletion is the most straightforward method of handling missing values. In this method, all instances with any missing values are removed. This method is useful when the proportion of missing data is low, and deleting the instances will not significantly impact the performance of the model. However, this method can lead to a loss of valuable data and reduced model performance.\n",
    "\n",
    "`Mean/median imputation`: In mean/median imputation, the missing values in a column are replaced by the mean or median of the other values in that column. This method is simple and easy to implement, but it can lead to biased results, especially if there are many missing values in the data.\n",
    "\n",
    "`Mode imputation`: Mode imputation is similar to mean/median imputation, except that the mode (most common value) is used to replace missing values in categorical data.\n",
    "\n",
    "`Hot deck imputation`: Hot deck imputation is a method in which the missing values are replaced with the value of the nearest neighbor. The nearest neighbor is selected based on the similarity of the other attributes. This method is useful when the data has a natural ordering, such as time series data.\n",
    "\n",
    "`Multiple imputation`: Multiple imputation is a method in which missing values are replaced with multiple estimates based on the relationships between the missing and non-missing values. Multiple imputation can provide better estimates of the missing data compared to single imputation methods.\n",
    "\n",
    "`Regression imputation`: Regression imputation is a method in which a regression model is used to predict the missing values based on the other attributes in the data. This method can provide accurate estimates of the missing values but requires a lot of computational resources.\n",
    "\n",
    "*In conclusion, there are several methods to handle missing data, and the choice of method depends on the nature and proportion of missing data, as well as the research question and available resources.*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Data pre-processing refers to a set of techniques used to transform and prepare raw data for analysis. Some of the common pre-processing techniques include cleaning, normalization, transformation, integration, and reduction.\n",
    "\n",
    "Dimensionality reduction is a technique used to reduce the number of features in a dataset while preserving as much information as possible. It is often used to address the curse of dimensionality, which occurs when datasets have too many features relative to the number of observations. Dimensionality reduction methods include principal component analysis (PCA), linear discriminant analysis (LDA), and t-distributed stochastic neighbor embedding (t-SNE).\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features from a dataset to use in the analysis. It is an important step in data pre-processing, as it can improve the accuracy and efficiency of machine learning models. Feature selection methods include filter methods, wrapper methods, and embedded methods. Filter methods rely on statistical tests to rank the importance of features, while wrapper methods use machine learning models to evaluate subsets of features. Embedded methods incorporate feature selection into the learning process of the machine learning model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.\n",
    "\n",
    "    i. What is the IQR? What criteria are used to assess it?\n",
    "\n",
    "    ii. Describe the various components of a box plot in detail? When will the lower whisker    surpass the upper whisker in length? How can box plots be used to identify outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "i. The IQR (Interquartile Range) is a measure of statistical dispersion that measures the spread of a dataset by dividing it into quarters. It is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) of the dataset. The IQR is used to identify the range of values that are most typical in the dataset, as well as to identify potential outliers. The criteria for assessing the IQR is that a large IQR indicates a wide dispersion of data, whereas a small IQR indicates that the data are closely clustered.\n",
    "\n",
    "The formula for calculating the IQR is:\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "where Q3 is the 75th percentile and Q1 is the 25th percentile.\n",
    "\n",
    "ii. A box plot is a graphical representation of a dataset that displays the distribution of the data through their quartiles. The various components of a box plot are as follows:\n",
    "\n",
    "The median line: This is a vertical line that divides the box into two halves, representing the median of the dataset.\n",
    "The box: This is a rectangle that spans the interquartile range (IQR), or the middle 50% of the data.\n",
    "The whiskers: These are vertical lines that extend from the box to the highest and lowest values within 1.5 times the IQR of the upper and lower quartiles, respectively.\n",
    "Outliers: These are data points that fall outside the whiskers.\n",
    "The lower whisker will surpass the upper whisker in length when the upper quartile is close to the median, and the lower quartile is far away from the median, indicating that there is a large dispersion of data in the lower half of the distribution.\n",
    "\n",
    "Box plots can be used to identify outliers by examining the data points that fall outside the whiskers. Any data points that are identified as outliers can be further investigated to determine if they are legitimate or if they are due to data entry errors or other issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Make brief notes on any two of the following:\n",
    "\n",
    "              1. Data collected at regular intervals\n",
    "\n",
    "               2. The gap between the quartiles\n",
    "\n",
    "               3. Use a cross-tab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "1. Data collected at regular intervals:\n",
    "\n",
    "This refers to the practice of collecting data at equal intervals of time or space. This approach is commonly used in time series analysis, where data is collected at regular time intervals, such as daily, weekly, or monthly, to study patterns or trends over time.\n",
    "\n",
    "2. The gap between the quartiles:\n",
    "\n",
    "The gap between the quartiles is also known as the interquartile range (IQR), which is a measure of variability in a dataset. It is calculated by subtracting the first quartile from the third quartile. The IQR is used to identify the middle 50% of the data and is less sensitive to outliers than the range.\n",
    "\n",
    "3. Use a cross-tab: \n",
    "\n",
    "A cross-tab, short for cross-tabulation, is a table that shows the relationship between two or more variables. It is commonly used to analyze categorical data by counting the number of observations in each category for two or more variables. Cross-tabs are useful for identifying patterns or trends in data, and can also be used to test hypotheses or examine the association between variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Make a comparison between:\n",
    "\n",
    "    1. Data with nominal and ordinal values\n",
    "\n",
    "    2. Histogram and box plot\n",
    "\n",
    "    3. The average and median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "1. Data with nominal and ordinal values:\n",
    "\n",
    "- `Nominal data` are categorical data without any order or ranking, such as gender or color.\n",
    "- `Ordinal data` are categorical data with a ranking or order, such as the education level or satisfaction rating.\n",
    "\n",
    "2. Histogram and box plot:\n",
    "\n",
    "- A `histogram` is a graphical representation of the distribution of numerical data, where data is split into bins or intervals and the count or frequency of data points in each bin is represented by the height of the bar.\n",
    "- A `box plot` (also known as a box and whisker plot) is a graphical representation of the distribution of numerical data based on quartiles, where the box represents the interquartile range (IQR), the median is shown as a line inside the box, and the whiskers extend to the minimum and maximum values, excluding any outliers.\n",
    "\n",
    "3. The average and median:\n",
    "\n",
    "- The `average` (also known as the mean) is a measure of central tendency that is calculated by adding all values and dividing by the total number of values. It is affected by outliers and extreme values.\n",
    "- The `median` is a measure of central tendency that represents the middle value when data is arranged in order. It is not affected by outliers or extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
