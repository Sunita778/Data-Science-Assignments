{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment_18\n",
    "Submitted by - Sunita Pradhan\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. What is the difference between supervised and unsupervised learning? Give some examples to illustrate your point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Supervised learning and unsupervised learning are two main categories of machine learning. The primary difference between these two types of learning is the presence or absence of labeled data.\n",
    "\n",
    "Supervised learning involves training a model on labeled data, where the input data has both the input features and the corresponding output labels. The goal of the model is to learn a mapping between the input features and the output labels, so that it can accurately predict the labels for new input data. Examples of supervised learning include image classification, speech recognition, and spam filtering.\n",
    "\n",
    "On the other hand, unsupervised learning involves training a model on unlabeled data, where the input data only has the input features without any corresponding output labels. The goal of the model is to discover underlying patterns or structure in the data, such as grouping similar data points into clusters or reducing the dimensionality of the data. Examples of unsupervised learning include clustering, dimensionality reduction, and anomaly detection.\n",
    "\n",
    "To illustrate the difference between supervised and unsupervised learning:\n",
    "\n",
    "- In supervised learning, a model might be trained to predict whether an email is spam or not based on labeled examples of emails that are either spam or not spam.\n",
    "- In unsupervised learning, a model might be used to cluster customer data into different groups based on patterns in their purchasing habits, without any prior knowledge of which group corresponds to a particular customer behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mention a few unsupervised learning applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "unsupervised learning is a powerful tool for discovering patterns and structure in data without the need for labeled examples, and has numerous applications in many different fields.\n",
    "\n",
    "- *Clustering*: Grouping similar data points together, based on their features or characteristics. This can be used in customer segmentation, market research, and image segmentation.\n",
    "\n",
    "- *Anomaly detection*: Identifying data points that are significantly different from the rest of the data. This can be used in fraud detection, network intrusion detection, and fault diagnosis.\n",
    "\n",
    "- *Dimensionality reduction*: Reducing the number of input features to a smaller set of more relevant features, while preserving most of the information. This can be used in data visualization, feature extraction, and image compression.\n",
    "\n",
    "- *Association rule learning*: Finding relationships and dependencies between variables or items in a dataset. This can be used in market basket analysis, recommendation systems, and customer behavior analysis.\n",
    "\n",
    "- *Generative modeling*: Learning the underlying distribution of the data and generating new data points that are similar to the original data. This can be used in image synthesis, text generation, and music composition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are the three main types of clustering methods? Briefly describe the characteristics of each.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "1. **Hierarchical clustering**: Hierarchical clustering builds a hierarchy of clusters, with the clusters at the bottom representing individual data points, and the clusters at higher levels representing groups of clusters. The algorithm can be either agglomerative or divisive. Agglomerative clustering starts with each data point in its own cluster and progressively merges clusters together until a stopping criterion is met. Divisive clustering starts with all data points in one cluster and recursively divides clusters until a stopping criterion is met. Hierarchical clustering does not require the number of clusters to be specified in advance and can produce a dendrogram to visualize the cluster hierarchy.\n",
    "\n",
    "2. **Partitioning clustering**: Partitioning clustering divides the data points into a predetermined number of non-overlapping clusters. The most popular partitioning algorithm is k-means, which initializes cluster centers randomly and then iteratively assigns each data point to the nearest cluster center and updates the cluster centers based on the new assignments. K-means is computationally efficient and easy to implement, but can be sensitive to the initial random initialization and is not guaranteed to find the globally optimal solution.\n",
    "\n",
    "3. **Density-based clustering**: Density-based clustering identifies clusters based on regions of high density in the data space. The most popular density-based clustering algorithm is DBSCAN, which requires two parameters: epsilon, which specifies the maximum distance between two points for them to be considered neighbors, and minPts, which specifies the minimum number of points required to form a dense region. DBSCAN is robust to noise and can handle clusters of arbitrary shape and size, but may have difficulty with clusters of varying densities or with widely varying scales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Explain how the k-means algorithm determines the consistency of clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The k-means algorithm determines the consistency of clustering by measuring the within-cluster sum of squared errors (SSE), also known as the sum of squared distances between each data point and its assigned cluster center. The SSE is a measure of how well the data points are clustered around their respective cluster centers.\n",
    "\n",
    "The k-means algorithm iteratively assigns each data point to the nearest cluster center and updates the cluster centers based on the new assignments, aiming to minimize the SSE. The algorithm continues until convergence, when the cluster assignments no longer change or the maximum number of iterations is reached.\n",
    "\n",
    "To evaluate the consistency of clustering, the k-means algorithm can be run multiple times with different random initializations, and the SSE values for each run can be compared. A smaller SSE indicates a better clustering result, but it is important to balance the SSE with the number of clusters and the interpretability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. With a simple illustration, explain the key difference between the k-means and k-medoids algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The key difference between k-means and k-medoids algorithms is that k-means uses the mean of the points in each cluster to determine the cluster centroid, while k-medoids uses the actual data points in the cluster as the centroid. K-medoids is more robust to outliers than k-means because it is less influenced by extreme values. However, k-medoids is also more computationally intensive than k-means because it requires calculating distances between each data point and all other data points in the cluster, while k-means only requires calculating the mean of the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What is a dendrogram, and how does it work? Explain how to do it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "A dendrogram is a tree-like diagram that shows the hierarchical relationship between objects. In data analysis, it is often used in hierarchical clustering to represent how similar or dissimilar different clusters or data points are to each other.\n",
    "\n",
    "To create a dendrogram, the following steps are typically followed:\n",
    "\n",
    "- Calculate the distance matrix between all data points or clusters using a chosen distance metric (e.g. Euclidean distance).\n",
    "- Merge the two closest data points or clusters into a new cluster.\n",
    "- Recalculate the distance matrix with the newly created cluster.\n",
    "- Repeat steps 2 and 3 until all data points or clusters are combined into a single cluster.\n",
    "- Plot the dendrogram, with the vertical axis representing the distance between clusters and the horizontal axis representing the individual data points or clusters.\n",
    "\n",
    "The resulting dendrogram will show the hierarchical structure of the data, with more similar data points or clusters being grouped together at lower levels, and less similar data points or clusters being grouped together at higher levels. The height of the branches in the dendrogram represents the distance between the clusters, with longer branches indicating greater dissimilarity. Dendrograms can be helpful in visualizing the structure of the data and identifying natural groupings or clusters within it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What exactly is SSE? What role does it play in the k-means algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "SSE stands for \"Sum of Squared Errors,\" and it is a measure of how well a k-means clustering algorithm is performing. Specifically, it measures the total sum of the squared distances between each data point and its assigned cluster centroid.\n",
    "\n",
    "In the k-means algorithm, the goal is to minimize SSE by iteratively adjusting the position of the cluster centroids until a local minimum is reached. The algorithm starts by randomly selecting k initial centroids, and then assigning each data point to the closest centroid. The centroids are then updated to the mean of the points assigned to them, and the process is repeated until convergence is reached (i.e., the centroids no longer move or the SSE no longer decreases).\n",
    "\n",
    "SSE plays an important role in k-means because it is used as the objective function to optimize. The algorithm tries to find the best possible clustering of the data points by minimizing the SSE. A lower SSE indicates that the data points are closer to their respective centroids, and thus the clusters are more tightly packed. Therefore, the k-means algorithm seeks to minimize the SSE in order to create clusters that are as compact and well-separated as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. With a step-by-step algorithm, explain the k-means procedure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The k-means algorithm is a popular clustering method that separates a dataset into k distinct clusters. Here is a step-by-step algorithm to explain the k-means procedure:\n",
    "\n",
    "- Choose the number of clusters k that you want to divide the dataset into.\n",
    "- Randomly select k points from the dataset as the initial centroids for each cluster.\n",
    "- Assign each data point in the dataset to the nearest centroid, creating k clusters.\n",
    "- Calculate the mean of each cluster to obtain a new centroid for each cluster.\n",
    "- Re-assign each data point to the closest centroid.\n",
    "- Repeat steps 4 and 5 until the centroids no longer move or a stopping criterion is reached (e.g., maximum number of iterations or minimum improvement in SSE).\n",
    "\n",
    "Here's a more detailed breakdown of the steps:\n",
    "\n",
    "1. Choose the number of clusters k that you want to divide the dataset into. This is often determined through prior knowledge of the problem or through methods like the elbow method or silhouette score.\n",
    "2. Randomly select k points from the dataset as the initial centroids for each cluster. This can be done by selecting k data points at random from the dataset or through more sophisticated initialization methods like k-means++.\n",
    "3. Assign each data point in the dataset to the nearest centroid, creating k clusters. This can be done by calculating the Euclidean distance between each data point and each centroid, and assigning each data point to the centroid with the minimum distance.\n",
    "4. Calculate the mean of each cluster to obtain a new centroid for each cluster. This involves taking the average of all the data points assigned to each cluster to obtain a new centroid for that cluster.\n",
    "5. Re-assign each data point to the closest centroid. This involves repeating step 3, but with the updated centroids from step 4.\n",
    "6. Repeat steps 4 and 5 until the centroids no longer move or a stopping criterion is reached. The algorithm terminates when the centroids no longer move or when a stopping criterion is reached, such as a maximum number of iterations or minimum improvement in SSE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. In the sense of hierarchical clustering, define the terms single link and complete link.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "Single link and complete link are two different types of linkage criteria used in hierarchical clustering to determine the distance between two clusters.\n",
    "\n",
    "Single link (also known as minimum linkage) calculates the distance between the two closest points in each cluster and considers that as the distance between the clusters. In other words, it measures the similarity between two clusters based on their closest pair of points. Single link tends to create long, elongated clusters as it only requires one point to be similar for the clusters to merge.\n",
    "\n",
    "On the other hand, complete link (also known as maximum linkage) calculates the distance between the two farthest points in each cluster and considers that as the distance between the clusters. In other words, it measures the similarity between two clusters based on their most dissimilar pair of points. Complete link tends to create compact, spherical clusters as it requires all points in both clusters to be similar for the clusters to merge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. How does the apriori concept aid in the reduction of measurement overhead in a business basket analysis? Give an example to demonstrate your point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ans:*\n",
    "\n",
    "The Apriori algorithm is a popular method used in market basket analysis, which aims to identify the most frequent itemsets in a transactional dataset. The Apriori algorithm uses the concept of association rule mining to find the correlations between the items in the dataset, and it does so by using a set of thresholds and constraints to prune the search space and avoid unnecessary computations.\n",
    "\n",
    "One of the main advantages of the Apriori algorithm is that it helps to reduce the measurement overhead in a business basket analysis by eliminating the need to examine all possible item combinations. Instead, the algorithm focuses only on the frequent itemsets that meet a minimum support threshold, which reduces the number of itemsets to be examined and thus the computational time and resources required.\n",
    "\n",
    "For example, consider a grocery store that wants to analyze its sales data to identify which products are often purchased together. By using the Apriori algorithm, the store can set a minimum support threshold, say 5%, and only examine the itemsets that occur in at least 5% of the transactions. This reduces the number of itemsets to be examined and allows the store to focus its attention on the most relevant itemsets.\n",
    "\n",
    "In this way, the Apriori algorithm can help businesses to identify patterns in their sales data more efficiently and effectively, which can lead to better insights and decision-making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
