{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf89628",
   "metadata": {},
   "source": [
    "## Submitted by : `Sunita Pradhan`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7288ef",
   "metadata": {},
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a)\tsigmoid\n",
    "b)\ttanh\n",
    "c)\tReLU\n",
    "d)\tELU\n",
    "e)\tLeakyReLU\n",
    "f)\tswish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e72f1c",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "a) Sigmoid:\n",
    "\n",
    "The sigmoid activation function is a mathematical function that takes any input value and maps it to a value between 0 and 1. It has an \"S\" shaped curve and is commonly used in binary classification problems. When the input is large, the sigmoid function outputs a value close to 1, indicating a high probability, and when the input is small, it outputs a value close to 0, indicating a low probability. The sigmoid function is useful for introducing non-linearity into neural networks and squashing the output into a manageable range.\n",
    "\n",
    "b) Tanh:\n",
    "\n",
    "The tanh (hyperbolic tangent) activation function is similar to the sigmoid function but maps the input values to a range between -1 and 1. It has an \"S\" shaped curve like the sigmoid function but is symmetric around the origin. The tanh function is commonly used in recurrent neural networks (RNNs) and can capture both positive and negative values, making it suitable for a wider range of problems.\n",
    "\n",
    "c) ReLU (Rectified Linear Unit):\n",
    "\n",
    "The ReLU activation function is a simple and widely used activation function. It returns the input value if it is positive, and 0 otherwise. In other words, it \"rectifies\" the negative values to 0. ReLU is computationally efficient and helps overcome the vanishing gradient problem that can occur with other activation functions. It is the most popular choice for activation functions in deep learning models due to its simplicity and ability to introduce non-linearity.\n",
    "\n",
    "d) ELU (Exponential Linear Unit):\n",
    "\n",
    "The ELU activation function is an extension of the ReLU function that addresses some of its limitations. It behaves like a linear function for positive inputs but smoothly curves for negative inputs, allowing the activation to have negative values. This helps prevent dead neurons and improves the gradient flow during training. The ELU function can handle negative values better than ReLU, but it is slightly more computationally expensive.\n",
    "\n",
    "e) LeakyReLU (Leaky Rectified Linear Unit):\n",
    "\n",
    "The LeakyReLU activation function is another variant of the ReLU function. It introduces a small slope for negative inputs, instead of setting them to 0 like ReLU. This small slope allows a small gradient flow for negative values, which can help alleviate the dying ReLU problem. The LeakyReLU function helps overcome the limitations of ReLU by preventing the complete death of neurons and enabling better training of deep neural networks.\n",
    "\n",
    "f) Swish:\n",
    "\n",
    "The Swish activation function is a recently proposed activation function that has gained popularity. It is a smooth and non-monotonic function that combines elements of the sigmoid and ReLU functions. The Swish function applies a sigmoid-like transformation to the input, but unlike sigmoid, it doesn't squash the output to a specific range. Instead, it allows the positive values to grow unbounded, providing more flexibility in capturing complex patterns. Swish has been observed to yield better results in some cases compared to ReLU, but its advantages are problem-dependent and may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225427e6",
   "metadata": {},
   "source": [
    "2. What happens when you increase or decrease the optimizer learning rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d72060",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "When you increase the optimizer learning rate, the algorithm takes larger steps during the optimization process. This can lead to faster convergence to the optimal solution, as the updates to the model's parameters are more significant. However, a high learning rate can also cause overshooting, where the algorithm may consistently miss the optimal solution and fail to converge. It can result in instability and difficulty in finding the global minimum of the loss function. On the other hand, when you decrease the learning rate, the optimization process becomes slower, but it can help the algorithm to converge more accurately and avoid overshooting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4553589",
   "metadata": {},
   "source": [
    "3. What happens when you increase the number of internal hidden neurons?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18812775",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Increasing the number of internal hidden neurons in a neural network allows the model to have a higher capacity for learning complex patterns and representations. With more hidden neurons, the network can capture more intricate and detailed relationships within the data. This increased capacity can potentially lead to improved performance and better accuracy on the training set. However, increasing the number of hidden neurons also increases the model's complexity, making it more prone to overfitting if not properly regularized. Additionally, having too many hidden neurons can result in longer training times and increased computational requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34e865",
   "metadata": {},
   "source": [
    "4. What happens when you increase the size of batch computation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449db471",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Increasing the size of batch computation refers to using larger mini-batches during the training process. A mini-batch is a subset of the training data that is processed together before updating the model's parameters. When the batch size is increased, the algorithm processes more data points simultaneously, which can lead to more stable and accurate parameter updates. It can also result in faster training times since the updates are performed in parallel. However, larger batch sizes require more memory and computational resources. Additionally, extremely large batch sizes may cause the model to generalize less effectively, as it might miss out on some of the finer details and variations present in smaller batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccdde8d",
   "metadata": {},
   "source": [
    "5. Why we adopt regularization to avoid overfitting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6f332",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Regularization is adopted to avoid overfitting in deep learning models. Overfitting occurs when a model learns to perform well on the training data but fails to generalize to new, unseen data. It happens when the model becomes too complex and starts to memorize the noise or irrelevant patterns present in the training data. Regularization techniques, such as L1 or L2 regularization, help prevent overfitting by adding a penalty term to the loss function. This penalty encourages the model to have smaller weights or simpler representations, reducing the model's tendency to overfit the training data. Regularization helps strike a balance between fitting the training data well and generalizing to unseen data, improving the model's overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4856f1",
   "metadata": {},
   "source": [
    "6. What are loss and cost functions in deep learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243a4771",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "In deep learning, the terms \"loss\" and \"cost\" functions are used interchangeably to refer to the function that quantifies how well the model's predictions match the true values or labels. The loss function measures the error or discrepancy between the predicted outputs and the actual outputs. It provides a numerical representation of the model's performance on a specific task, such as classification or regression. The goal of training a deep learning model is to minimize this loss function, typically using optimization algorithms like gradient descent. By iteratively adjusting the model's parameters based on the loss function, the model can improve its predictions and learn to generalize better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffec0af1",
   "metadata": {},
   "source": [
    "7. What do ou mean by underfitting in neural networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b390a29",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Underfitting in neural networks refers to a situation where the model fails to capture the underlying patterns and complexities present in the training data. It occurs when the model's capacity is too low or when it is not trained for a sufficient number of iterations. An underfit model tends to have high bias and low variance. It exhibits poor performance on both the training data and new, unseen data. In an underfitting scenario, the model is overly simplistic and fails to capture the intricacies and nuances of the data, resulting in low accuracy or poor predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc202555",
   "metadata": {},
   "source": [
    "8. Why we use Dropout in Neural Networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6282c",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Dropout is used in neural networks as a regularization technique to prevent overfitting. It involves randomly \"dropping out\" a proportion of the neurons or connections in a neural network during each training iteration. By doing so, dropout forces the network to learn redundant representations, as different subsets of neurons are activated during each training step. This prevents any single neuron from relying too heavily on the presence of specific other neurons. Dropout helps improve the generalization ability of the model by reducing the interdependencies between neurons and promoting more robust and diverse feature representations. It acts as a form of ensemble learning within a single model, as multiple different subnetworks are effectively trained and combined during training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86177455",
   "metadata": {},
   "source": [
    "### Thank youðŸ˜Š"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
