{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Assignment 3\n",
    "Submitted by - Sunita Pradhan\n",
    "\n",
    "-----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Write the Gaussian Distribution empirical formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "The Gaussian or normal distribution is a continuous probability distribution that is defined by the following empirical formula:\n",
    "\n",
    "f(x) = (1 / (σ * √(2π))) * e^(-(x - μ)^2 / 2σ^2)\n",
    "\n",
    "where:\n",
    "- μ is the mean or expectation of the distribution\n",
    "- σ is the standard deviation of the distribution\n",
    "- π is the mathematical constant pi (approximately equal to 3.14)\n",
    "- e is the mathematical constant e (approximately equal to 2.718)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What is the Z-score, and why is it important?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "The Z-score, also known as the standard score, is a measure of how many standard deviations a data point is from the mean of a data set. The Z-score is calculated as follows:\n",
    "\n",
    "Z = (X - μ) / σ\n",
    "\n",
    "where:\n",
    "- X is the value of the data point\n",
    "- μ is the mean of the data set\n",
    "- σ is the standard deviation of the data set\n",
    "The Z-score provides a way to standardize the data and compare it to a standard normal distribution with a mean of 0 and a standard deviation of 1. This allows you to determine whether a data point is unusual or extreme in comparison to the rest of the data.\n",
    "\n",
    "The importance of Z-score:\n",
    "- Outlier detection: By calculating the Z-score of each data point, you can identify outliers, or data points that are significantly different from the rest of the data. Outliers can have a significant impact on the results of statistical analysis, so it is important to identify and understand them.\n",
    "- Probability calculations: The Z-score allows you to calculate the probability of observing a certain value based on the standard normal distribution. This can be useful in hypothesis testing, where you want to determine the likelihood of observing certain results given a certain assumption.\n",
    "- Normalization: By standardizing the data to a Z-score, you can compare data from different data sets that have different units of measurement or different scales. This allows you to compare the data on a common scale and make meaningful comparisons.\n",
    "- Data interpretation: The Z-score provides a way to interpret the data in terms of standard deviations from the mean. This can be useful for identifying trends and patterns in the data, and for making predictions about future data based on the distribution of the data.\n",
    "\n",
    "In summary, the Z-score is an important tool in statistics that provides a way to standardize the data, calculate probabilities, detect outliers, and make meaningful comparisons between data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What is an outlier, exactly?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "An outlier is an observation in a data set that is significantly different from other observations in the same data set. An outlier can be defined in different ways, but it is generally considered to be an observation that lies outside of the expected range of values for the data set.\n",
    "\n",
    "Outliers can arise due to several reasons, including measurement errors, data entry errors, or a true difference in the underlying population being studied. Outliers can have a significant impact on the results of statistical analysis, so it is important to identify and understand them.\n",
    "\n",
    "There are several methods for identifying outliers in a data set, including the use of statistical measures, such as the Z-score or the interquartile range (IQR), as well as visual methods, such as box plots or scatter plots. The choice of method depends on the nature of the data and the goals of the analysis.\n",
    "\n",
    "It is important to note that outliers should not be automatically removed from a data set, as they can contain important information about the underlying population. Instead, outliers should be carefully evaluated and either justified or removed only after considering their impact on the analysis. In some cases, outliers can indicate a subpopulation within the data that deserves further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What are our options for dealing with outliers in our dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "- **Keep the outliers**: In some cases, outliers may be legitimate and meaningful data points that should not be removed. For example, if analyzing data from a medical study, some extreme values may represent rare but important health conditions. In such cases, it may be appropriate to keep the outliers in the dataset and adjust the analysis accordingly.\n",
    "\n",
    "- **Remove the outliers**: In other cases, outliers may be the result of measurement errors, data entry errors, or other types of noise or bias. Removing outliers can help to reduce the impact of these errors on the analysis and improve the accuracy of the results. However, this approach should be used with caution and should be based on careful evaluation of the data and the reasons for the outliers.\n",
    "\n",
    "- **Transform the data**: Outliers can also be addressed by transforming the data, such as by taking the logarithm, square root, or other mathematical functions of the data. This can help to reduce the influence of extreme values on the analysis and bring the data closer to a normal distribution.\n",
    "\n",
    "- **Winsorize the data**: Winsorization is a technique that involves replacing extreme values with less extreme values. For example, the top 1% of values in a dataset can be replaced with the 99th percentile value, and the bottom 1% of values can be replaced with the 1st percentile value. This can help to reduce the impact of outliers while still preserving some of the information in the data.\n",
    "\n",
    "- **Use robust statistical methods**: Robust statistical methods are designed to be less sensitive to outliers and other types of extreme values. For example, instead of calculating the mean, which can be heavily influenced by outliers, a robust method such as the median or trimmed mean can be used.\n",
    "\n",
    "It's important to note that each of these options has its own strengths and weaknesses, and the best approach will depend on the specific dataset and analysis at hand. It's also important to carefully document any decisions made regarding outlier handling, as these decisions can have a significant impact on the results and interpretations of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Write the sample and population variances equations and explain Bessel Correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Sample variance is a measure of the spread or variability of a set of data points around their mean, and it is calculated using the following formula:\n",
    "\n",
    "s^2 = Σ(xi - x̄)^2 / (n - 1)\n",
    "\n",
    "where s^2 is the sample variance, xi is the ith data point, x̄ is the sample mean, and n is the sample size.\n",
    "\n",
    "Population variance, on the other hand, is a measure of the spread or variability of a population of data points around their mean, and it is calculated using the following formula:\n",
    "\n",
    "σ^2 = Σ(xi - μ)^2 / N\n",
    "\n",
    "where σ^2 is the population variance, xi is the ith data point, μ is the population mean, and N is the population size.\n",
    "\n",
    "The Bessel correction is a mathematical adjustment that is applied to the sample variance formula in order to make it an unbiased estimator of the population variance. This correction is necessary because the sample variance tends to underestimate the population variance, especially for small sample sizes. The Bessel correction adjusts for the fact that the sample mean is used in place of the population mean, and it is calculated by dividing the sum of squared deviations by (n - 1) instead of n:\n",
    "\n",
    "s^2 = Σ(xi - x̄)^2 / (n - 1)\n",
    "\n",
    "By using (n - 1) instead of n in the denominator, the Bessel correction slightly increases the value of the sample variance, which makes it a better estimate of the population variance. The Bessel correction is named after Friedrich Bessel, a German mathematician who first proposed the correction in the 19th century."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
